{"cells":[{"cell_type":"markdown","metadata":{"id":"EAiHVEoWHy_D"},"source":["# Deep Convolutional Q-Learning for Pac-Man"]},{"cell_type":"markdown","metadata":{"id":"tjO1aK3Ddjs5"},"source":["## Part 0 - Installing the required packages and importing the libraries"]},{"cell_type":"markdown","metadata":{"id":"NwdRB-ZLdrAV"},"source":["### Installing Gymnasium"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dbnq3XpoKa_7"},"outputs":[],"source":["# !pip install gymnasium\n","# !pip install \"gymnasium[atari, accept-rom-license]\"\n","# !apt-get install -y swig\n","# !pip install gymnasium[box2d]"]},{"cell_type":"markdown","metadata":{"id":"H-wes4LZdxdd"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ho_25-9_9qnu"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import deque\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","metadata":{"id":"m7wa0ft8e3M_"},"source":["## Part 1 - Building the AI"]},{"cell_type":"markdown","metadata":{"id":"dlYVpVdHe-i6"},"source":["### Creating the architecture of the Neural Network"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class Network(nn.Module):\n","    def __init__(self, action_size, seed = 42):\n","        super(Network, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size = 8, stride = 4)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1)\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, action_size)\n","        \n","    def forward(self, state: torch.Tensor) -> torch.Tensor:\n","        x = F.relu(self.bn1(self.conv1(state)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.relu(self.bn4(self.conv4(x)))        \n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))        \n","        return self.fc3(x)"]},{"cell_type":"markdown","metadata":{"id":"rUvCfE_mhwo2"},"source":["## Part 2 - Training the AI"]},{"cell_type":"markdown","metadata":{"id":"WWCDPF22lkwc"},"source":["### Setting up the environment"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["State shape:  (210, 160, 3)\n","State Size:  210\n","Number of actions:  9\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mufaddalhamid/Documents/Learn/AI_Course/.conda/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.deprecation(\n","A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n","[Powered by Stella]\n"]}],"source":["import gymnasium as gym\n","\n","env = gym.make(\"MsPacmanDeterministic-v0\", full_action_space=False)\n","state_shape = env.observation_space.shape\n","state_size = state_shape[0]\n","no_of_actions = env.action_space.n\n","\n","print(\"State shape: \", state_shape)\n","print(\"State Size: \", state_size)\n","print(\"Number of actions: \", no_of_actions)"]},{"cell_type":"markdown","metadata":{"id":"Bx6IdX3ciDqH"},"source":["### Initializing the hyperparameters"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["learning_rate = 5e-4\n","minibatch_size = 64\n","gamma = 0.99  # discount factor"]},{"cell_type":"markdown","metadata":{"id":"U2bDShIEkA5V"},"source":["### Preprocessing the frames"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","\n","\n","def preprocess_frame(frame):\n","    frame = Image.fromarray(frame)\n","    preprocess = transforms.Compose([\n","        transforms.Resize((128, 128)),\n","        transforms.ToTensor(),\n","    ])\n","    \n","    return preprocess(frame).unsqueeze(0)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"imMdSO-HAWra"},"source":["### Implementing the DCQN class"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class Agent:\n","    def __init__(self, action_size):\n","        self.device = torch.device(\"cpu\")\n","        self.action_size = action_size\n","        self.local_qnetwork = Network(action_size).to(self.device)\n","        self.target_qnetwork = Network(action_size).to(self.device)\n","        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n","        self.memory = deque(maxlen=10000)\n","\n","    def step(self, state, action, reward, next_state, done) -> None:\n","        state = preprocess_frame(state)\n","        next_state = preprocess_frame(next_state)\n","        self.memory.append((state, action, reward, next_state, done))\n","        if len(self.memory) > minibatch_size:\n","            experiences = random.sample(self.memory, minibatch_size)\n","            self.learn(experiences, gamma)\n","\n","    def act(self, state, eps=0.0) -> int:\n","        \"\"\"Returns actions for given state as per current policy.\"\"\"\n","        state = preprocess_frame(state).to(self.device)\n","        self.local_qnetwork.eval()\n","        with torch.no_grad():\n","            action_values = self.local_qnetwork(state)\n","        self.local_qnetwork.train()\n","        # Epsilon-greedy action selection\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","\n","    def learn(self, experiences, discount_factor):\n","        states, actions, rewards, next_states, dones = zip(*experiences)\n","        states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n","        actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n","        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n","        next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n","        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n","        next_q_targets = (\n","            self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n","        )\n","        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n","        q_expected = self.local_qnetwork(states).gather(1, actions)\n","        loss = F.mse_loss(q_expected, q_targets)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"yUg95iBpAwII"},"source":["### Initializing the DCQN agent"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["agent = Agent(no_of_actions)"]},{"cell_type":"markdown","metadata":{"id":"CK6Zt_gNmHvm"},"source":["### Training the DCQN agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["no_of_episodes = 2000\n","max_timesteps = 10000  # max number of timesteps in an episode\n","epsilon_start = 1.0\n","epsilon_end = 0.01\n","epsilon_decay = 0.995\n","eps = epsilon_start\n","scores = deque(maxlen=100)  # list containing scores from last 100 episodes\n","\n","for episode in range(1, no_of_episodes + 1):\n","    state, _ = env.reset()\n","    score = 0\n","    for t in range(max_timesteps):\n","        action = agent.act(state, eps)\n","        next_state, reward, done, _, _ = env.step(action)\n","        agent.step(state, action, reward, next_state, done)\n","        state = next_state\n","        score += reward\n","        if done:\n","            break\n","    scores.append(score)\n","    eps = max(epsilon_end, epsilon_decay * eps)\n","    print(f\"\\rEpisode {episode}\\tAverage Score: {np.mean(scores):.2f}\\tCurrent Score: {score:.2f}    \", end=\"\")\n","    if episode % 100 == 0:\n","        print(f\"\\rEpisode {episode}\\tAverage Score: {np.mean(scores):.2f}                          \")\n","    if np.mean(scores) >= 500.0:\n","        print(f\"\\nEnvironment solved in {episode:d} episodes!\\tAverage Score: {np.mean(scores):.2f}\")\n","        torch.save(agent.local_qnetwork.state_dict(), \"checkpoint.pth\")\n","        break"]},{"cell_type":"markdown","metadata":{"id":"-0WhhBV8nQdf"},"source":["## Part 3 - Visualizing the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb9nVvU2Okhk"},"outputs":[],"source":["import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n","\n","def show_video_of_model(agent, env_name):\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    state, _ = env.reset()\n","    done = False\n","    frames = []\n","    while not done:\n","        frame = env.render()\n","        frames.append(frame)\n","        action = agent.act(state)\n","        state, reward, done, _, _ = env.step(action)\n","    env.close()\n","    imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","show_video()"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1nqb-KnVe1EsZF-03Iba7T3cZFsnVRl4H","timestamp":1695853702757}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
